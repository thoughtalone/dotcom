# AI isn't Artifical Intelligence
### DRAFT

## Definitions

There are two defintions of AI: (1) Weak AI and (2) Strong AI. 

Weak AI is pseudo-intelligent: a system is weakly AI if it is able to automate a specific task, which in humans, requires intelligence. 

Strong AI is intelligent: a system is strongly AI if it acquires a general skill at a range of tasks where improving ability on one tasks, raises ability across all related tasks. 

## Automation is not Artifical Intelligence

A task may require intelligence, in animals, to perform: say, sorting books into genres. This requires a background knowledge of genre, literary style, authorship. It requires a certain creativity in determination of edge cases, the application of intution and experience. 

That this task can be performed by a machine does *not* imply that the machine adopts the same strategy, ie., that it uses intelligence in its solution. 

For example, in many cases books of certain genres will have a common visual style to their covers: by sorting on pixel patterns alone a system could achive a high degree of accuracy; perhaps matching that of a human sorter.

Nevertheless the machine adopts a pseudo-intelligent strategy, that is, one which merely appears intelligent in its output. 

For most of human history tools have played this role: a train upon a track *navigates* land without intelligence; yet a cartographer requires it. 

A rock rolling down a hill finds, of its own accord, a very direct route to the bottom. The rock isn't especially capable of creative thought. 

## Neural Networks are Neither Neural nor Networks

Computer science has a self-important habit of regarding automation of the first kind (eliminating the need for intelligence) as an accomplishment in the second (creating an intelligent system).

It is a remarkable achievement to eliminate the need for intelligence: the time the librarian spends sorting books can be best spent elsewhere. However it's a great research catastrophie to, therefore, interrigate a pixel pattern algorithm for the concetual structure of science fiction. And likewise to turn to neural networks to explain the brain.

The "neural network algorithm" is a generalized form of linear regression, of the kind high school students perform with a pencil and graph paper <sup>[1](footnote-1)</sup>. 

Neurones in the brain are plastic: they rearrange themselves. They have activations patterns which are not fixed over time. They engage in biochemical as well as electrical signalling. There is no homology between the structure of a neural network and any brain structure, the brain does not "tune activation patterns" in the manner of fitting a line to data. 

## Intelligence is not (merely) Cognitive

Cognition can be modelled as a logical flow of "thoughts": (IF proposition THEN proposition).

However most animal mental functioning is non-cognitive. Our mental life is not simply propositional: I do not love my pet because of some inference. 

Most *behaviour* is intelligent because it has been conditioned by skill acquisition which is mostly non-cognitive. The bike rider, piano player, great writer does not "infer" what to do next from a sequence of antecedent thoughts.

Rather the body of the rider has been conditioned, that is, deeply restructured -- from muscles to brain -- in the face of increasingly skillfull bike riding. At no point was the rider cognitively aware that their behaviour could be characterized by the inference, "the wind is 30mph; if the wind is 30mph i should tilt 5deg; so i should tilt 5deg".

And importantly, though any specific action of the rider could be characterized -- extrinsically -- that way, the sum of his actions quickly leads to infinities: his body is capable of responding intelligently to an infinite variety of situations without possessing an infinite number of rules. 



### Footnotes

<a name="footnote-1">[1]</a> The NN algorithm is a parameter estimation procedure for fixing constant terms in a piece-wise linear regression model. The model has the form $max(0, W_1n \dots max(0, W_0X))$ where the $X$ is the 'network input' and each action of a `max(0, ...)` is a layer whose output is determined by the matrix-vector product $WX$.

This model has no plausible correspondance with anything in the brain. Though `max(0, wx)` is referred to as an "activation" a better term would be "score", and is a "score" only insofar as a dot product between two vectors describes the degree to which they "express a commonality". In this sense *any* application of a dot product is a "activation", which most equations in physics. 

